|    entropy_loss         | -0.861      |
|    explained_variance   | 0.878       |
|    learning_rate        | 0.00033     |
|    loss                 | 0.0972      |
|    n_updates            | 664         |
|    policy_gradient_loss | 0.000913    |
|    std                  | 0.27        |
|    value_loss           | 0.163       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 168         |
|    time_elapsed         | 5555        |
|    total_timesteps      | 2752512     |
| train/                  |             |
|    approx_kl            | 0.022636019 |
|    clip_fraction        | 0.363       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.85       |
|    explained_variance   | 0.837       |
|    learning_rate        | 0.000329    |
|    loss                 | 0.0627      |
|    n_updates            | 668         |
|    policy_gradient_loss | 0.00222     |
|    std                  | 0.269       |
|    value_loss           | 0.152       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 169         |
|    time_elapsed         | 5584        |
|    total_timesteps      | 2768896     |
| train/                  |             |
|    approx_kl            | 0.020414324 |
|    clip_fraction        | 0.347       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.825      |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.000328    |
|    loss                 | 0.073       |
|    n_updates            | 672         |
|    policy_gradient_loss | 0.00207     |
|    std                  | 0.268       |
|    value_loss           | 0.149       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 496         |
|    iterations           | 170         |
|    time_elapsed         | 5612        |
|    total_timesteps      | 2785280     |
| train/                  |             |
|    approx_kl            | 0.013522983 |
|    clip_fraction        | 0.308       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.804      |
|    explained_variance   | 0.924       |
|    learning_rate        | 0.000327    |
|    loss                 | 0.0659      |
|    n_updates            | 676         |
|    policy_gradient_loss | 0.00123     |
|    std                  | 0.268       |
|    value_loss           | 0.122       |
-----------------------------------------

---------------------------------------
Using our CustomActorCriticPolicy!
---------------------------------------


---------------------------------------
Starting model evaluation, checkpoint 28
---------------------------------------


---------------------------------------
Deleting Model:
Min walked distance: 1m
Mean walked distance: 10m
Mean step reward: 0.7132851362228394
Runs below 20m: [ 1  3  7 11 13 15 19]
---------------------------------------

-----------------------------------------
| time/                   |             |
|    fps                  | 492         |
|    iterations           | 171         |
|    time_elapsed         | 5690        |
|    total_timesteps      | 2801664     |
| train/                  |             |
|    approx_kl            | 0.018594336 |
|    clip_fraction        | 0.339       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.787      |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.000326    |
|    loss                 | 0.0574      |
|    n_updates            | 680         |
|    policy_gradient_loss | 0.000911    |
|    std                  | 0.267       |
|    value_loss           | 0.153       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 492         |
|    iterations           | 172         |
|    time_elapsed         | 5717        |
|    total_timesteps      | 2818048     |
| train/                  |             |
|    approx_kl            | 0.016932871 |
|    clip_fraction        | 0.297       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.764      |
|    explained_variance   | 0.916       |
|    learning_rate        | 0.000325    |
|    loss                 | 0.0718      |
|    n_updates            | 684         |
|    policy_gradient_loss | 0.000909    |
|    std                  | 0.266       |
|    value_loss           | 0.15        |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 493        |
|    iterations           | 173        |
|    time_elapsed         | 5746       |
|    total_timesteps      | 2834432    |
| train/                  |            |
|    approx_kl            | 0.01497392 |
|    clip_fraction        | 0.316      |
|    clip_range           | 0.15       |
|    clip_range_vf        | 0.15       |
|    entropy_loss         | -0.748     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.000324   |
|    loss                 | 0.0469     |
|    n_updates            | 688        |
|    policy_gradient_loss | 0.000151   |
|    std                  | 0.266      |
|    value_loss           | 0.109      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 493         |
|    iterations           | 174         |
|    time_elapsed         | 5773        |
|    total_timesteps      | 2850816     |
| train/                  |             |
|    approx_kl            | 0.015077727 |
|    clip_fraction        | 0.312       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.732      |
|    explained_variance   | 0.909       |
|    learning_rate        | 0.000323    |
|    loss                 | 0.0596      |
|    n_updates            | 692         |
|    policy_gradient_loss | 0.00139     |
|    std                  | 0.265       |
|    value_loss           | 0.146       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 494         |
|    iterations           | 175         |
|    time_elapsed         | 5801        |
|    total_timesteps      | 2867200     |
| train/                  |             |
|    approx_kl            | 0.014938098 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.713      |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.000322    |
|    loss                 | 0.073       |
|    n_updates            | 696         |
|    policy_gradient_loss | 0.00132     |
|    std                  | 0.265       |
|    value_loss           | 0.101       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 494         |
|    iterations           | 176         |
|    time_elapsed         | 5829        |
|    total_timesteps      | 2883584     |
| train/                  |             |
|    approx_kl            | 0.016331626 |
|    clip_fraction        | 0.323       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.699      |
|    explained_variance   | 0.839       |
|    learning_rate        | 0.000321    |
|    loss                 | 0.107       |
|    n_updates            | 700         |
|    policy_gradient_loss | 0.00109     |
|    std                  | 0.264       |
|    value_loss           | 0.142       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 495         |
|    iterations           | 177         |
|    time_elapsed         | 5857        |
|    total_timesteps      | 2899968     |
| train/                  |             |
|    approx_kl            | 0.015552857 |
|    clip_fraction        | 0.327       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.68       |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.00032     |
|    loss                 | 0.0636      |
|    n_updates            | 704         |
|    policy_gradient_loss | -0.000229   |
|    std                  | 0.264       |
|    value_loss           | 0.132       |
-----------------------------------------

---------------------------------------
Using our CustomActorCriticPolicy!
---------------------------------------


---------------------------------------
Starting model evaluation, checkpoint 29
---------------------------------------


---------------------------------------
Deleting Model:
Min walked distance: 2m
Mean walked distance: 14m
Mean step reward: 0.7855499267578125
Runs below 20m: [ 1  3  7 13 19]
---------------------------------------

-----------------------------------------
| time/                   |             |
|    fps                  | 490         |
|    iterations           | 178         |
|    time_elapsed         | 5946        |
|    total_timesteps      | 2916352     |
| train/                  |             |
|    approx_kl            | 0.015405336 |
|    clip_fraction        | 0.333       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.659      |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.000319    |
|    loss                 | 0.122       |
|    n_updates            | 708         |
|    policy_gradient_loss | 0.000595    |
|    std                  | 0.263       |
|    value_loss           | 0.182       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 490         |
|    iterations           | 179         |
|    time_elapsed         | 5974        |
|    total_timesteps      | 2932736     |
| train/                  |             |
|    approx_kl            | 0.011572874 |
|    clip_fraction        | 0.266       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.642      |
|    explained_variance   | 0.933       |
|    learning_rate        | 0.000318    |
|    loss                 | 0.0844      |
|    n_updates            | 712         |
|    policy_gradient_loss | 0.000164    |
|    std                  | 0.262       |
|    value_loss           | 0.105       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 491         |
|    iterations           | 180         |
|    time_elapsed         | 6002        |
|    total_timesteps      | 2949120     |
| train/                  |             |
|    approx_kl            | 0.014090069 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.627      |
|    explained_variance   | 0.62        |
|    learning_rate        | 0.000317    |
|    loss                 | 0.0613      |
|    n_updates            | 716         |
|    policy_gradient_loss | 0.000753    |
|    std                  | 0.262       |
|    value_loss           | 0.154       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 491         |
|    iterations           | 181         |
|    time_elapsed         | 6031        |
|    total_timesteps      | 2965504     |
| train/                  |             |
|    approx_kl            | 0.011579702 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.611      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.000316    |
|    loss                 | 0.0577      |
|    n_updates            | 720         |
|    policy_gradient_loss | -0.000151   |
|    std                  | 0.261       |
|    value_loss           | 0.125       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 492         |
|    iterations           | 182         |
|    time_elapsed         | 6059        |
|    total_timesteps      | 2981888     |
| train/                  |             |
|    approx_kl            | 0.013441535 |
|    clip_fraction        | 0.306       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.59       |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.000315    |
|    loss                 | 0.0984      |
|    n_updates            | 724         |
|    policy_gradient_loss | 0.00143     |
|    std                  | 0.26        |
|    value_loss           | 0.141       |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 492        |
|    iterations           | 183        |
|    time_elapsed         | 6088       |
|    total_timesteps      | 2998272    |
| train/                  |            |
|    approx_kl            | 0.01768608 |
|    clip_fraction        | 0.342      |
|    clip_range           | 0.15       |
|    clip_range_vf        | 0.15       |
|    entropy_loss         | -0.563     |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.000314   |
|    loss                 | 0.08       |
|    n_updates            | 728        |
|    policy_gradient_loss | 0.00126    |
|    std                  | 0.26       |
|    value_loss           | 0.135      |
----------------------------------------

---------------------------------------
Using our CustomActorCriticPolicy!
---------------------------------------


---------------------------------------
Starting model evaluation, checkpoint 30
---------------------------------------


---------------------------------------
Deleting Model:
Min walked distance: 1m
Mean walked distance: 11m
Mean step reward: 0.7024006128311158
Runs below 20m: [ 1  3  7 13 15 17 19]
---------------------------------------

----------------------------------------
| time/                   |            |
|    fps                  | 488        |
|    iterations           | 184        |
|    time_elapsed         | 6165       |
|    total_timesteps      | 3014656    |
| train/                  |            |
|    approx_kl            | 0.01858777 |
|    clip_fraction        | 0.378      |
|    clip_range           | 0.15       |
|    clip_range_vf        | 0.15       |
|    entropy_loss         | -0.54      |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.000313   |
|    loss                 | 0.078      |
|    n_updates            | 732        |
|    policy_gradient_loss | 0.00134    |
|    std                  | 0.259      |
|    value_loss           | 0.159      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 185         |
|    time_elapsed         | 6193        |
|    total_timesteps      | 3031040     |
| train/                  |             |
|    approx_kl            | 0.013638591 |
|    clip_fraction        | 0.318       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.521      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.000312    |
|    loss                 | 0.0765      |
|    n_updates            | 736         |
|    policy_gradient_loss | 0.000116    |
|    std                  | 0.259       |
|    value_loss           | 0.142       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 186         |
|    time_elapsed         | 6221        |
|    total_timesteps      | 3047424     |
| train/                  |             |
|    approx_kl            | 0.016723715 |
|    clip_fraction        | 0.354       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.509      |
|    explained_variance   | 0.302       |
|    learning_rate        | 0.000311    |
|    loss                 | 0.0993      |
|    n_updates            | 740         |
|    policy_gradient_loss | 0.00142     |
|    std                  | 0.258       |
|    value_loss           | 0.161       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 490         |
|    iterations           | 187         |
|    time_elapsed         | 6250        |
|    total_timesteps      | 3063808     |
| train/                  |             |
|    approx_kl            | 0.018031787 |
|    clip_fraction        | 0.369       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.491      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.00031     |
|    loss                 | 0.06        |
|    n_updates            | 744         |
|    policy_gradient_loss | 0.000494    |
|    std                  | 0.257       |
|    value_loss           | 0.141       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 490         |
|    iterations           | 188         |
|    time_elapsed         | 6277        |
|    total_timesteps      | 3080192     |
| train/                  |             |
|    approx_kl            | 0.020658264 |
|    clip_fraction        | 0.356       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.471      |
|    explained_variance   | 0.532       |
|    learning_rate        | 0.000309    |
|    loss                 | 0.089       |
|    n_updates            | 748         |
|    policy_gradient_loss | 0.00221     |
|    std                  | 0.257       |
|    value_loss           | 0.167       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 491         |
|    iterations           | 189         |
|    time_elapsed         | 6306        |
|    total_timesteps      | 3096576     |
| train/                  |             |
|    approx_kl            | 0.017553974 |
|    clip_fraction        | 0.348       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.451      |
|    explained_variance   | 0.928       |
|    learning_rate        | 0.000308    |
|    loss                 | 0.0588      |
|    n_updates            | 752         |
|    policy_gradient_loss | 0.0015      |
|    std                  | 0.256       |
|    value_loss           | 0.101       |
-----------------------------------------

---------------------------------------
Using our CustomActorCriticPolicy!
---------------------------------------


---------------------------------------
Starting model evaluation, checkpoint 31
---------------------------------------


---------------------------------------
Deleting Model:
Min walked distance: 1m
Mean walked distance: 11m
Mean step reward: 0.718921172618866
Runs below 20m: [ 1  3  7 13 15 19]
---------------------------------------

----------------------------------------
| time/                   |            |
|    fps                  | 487        |
|    iterations           | 190        |
|    time_elapsed         | 6384       |
|    total_timesteps      | 3112960    |
| train/                  |            |
|    approx_kl            | 0.01848809 |
|    clip_fraction        | 0.386      |
|    clip_range           | 0.15       |
|    clip_range_vf        | 0.15       |
|    entropy_loss         | -0.434     |
|    explained_variance   | 0.144      |
|    learning_rate        | 0.000307   |
|    loss                 | 0.0557     |
|    n_updates            | 756        |
|    policy_gradient_loss | 0.00295    |
|    std                  | 0.256      |
|    value_loss           | 0.165      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 487         |
|    iterations           | 191         |
|    time_elapsed         | 6413        |
|    total_timesteps      | 3129344     |
| train/                  |             |
|    approx_kl            | 0.016476342 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.412      |
|    explained_variance   | 0.854       |
|    learning_rate        | 0.000306    |
|    loss                 | 0.0682      |
|    n_updates            | 760         |
|    policy_gradient_loss | 0.00104     |
|    std                  | 0.255       |
|    value_loss           | 0.115       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 192         |
|    time_elapsed         | 6442        |
|    total_timesteps      | 3145728     |
| train/                  |             |
|    approx_kl            | 0.019672401 |
|    clip_fraction        | 0.366       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.388      |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.000305    |
|    loss                 | 0.0705      |
|    n_updates            | 764         |
|    policy_gradient_loss | 0.000602    |
|    std                  | 0.254       |
|    value_loss           | 0.181       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 488         |
|    iterations           | 193         |
|    time_elapsed         | 6470        |
|    total_timesteps      | 3162112     |
| train/                  |             |
|    approx_kl            | 0.016098427 |
|    clip_fraction        | 0.345       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.37       |
|    explained_variance   | 0.923       |
|    learning_rate        | 0.000304    |
|    loss                 | 0.0709      |
|    n_updates            | 768         |
|    policy_gradient_loss | 0.00202     |
|    std                  | 0.254       |
|    value_loss           | 0.116       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 194         |
|    time_elapsed         | 6498        |
|    total_timesteps      | 3178496     |
| train/                  |             |
|    approx_kl            | 0.022500671 |
|    clip_fraction        | 0.382       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.349      |
|    explained_variance   | 0.17        |
|    learning_rate        | 0.000303    |
|    loss                 | 0.0435      |
|    n_updates            | 772         |
|    policy_gradient_loss | 0.00111     |
|    std                  | 0.253       |
|    value_loss           | 0.138       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 489         |
|    iterations           | 195         |
|    time_elapsed         | 6528        |
|    total_timesteps      | 3194880     |
| train/                  |             |
|    approx_kl            | 0.017999114 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.000302    |
|    loss                 | 0.0796      |
|    n_updates            | 776         |
|    policy_gradient_loss | -0.000157   |
|    std                  | 0.252       |
|    value_loss           | 0.15        |
-----------------------------------------

---------------------------------------
Using our CustomActorCriticPolicy!
---------------------------------------


---------------------------------------
Starting model evaluation, checkpoint 32
---------------------------------------


---------------------------------------
Deleting Model:
Min walked distance: 1m
Mean walked distance: 12m
Mean step reward: 0.731348204612732
Runs below 20m: [ 1  3  7  9 11 15 17 19]
---------------------------------------

-----------------------------------------
| time/                   |             |
|    fps                  | 485         |
|    iterations           | 196         |
|    time_elapsed         | 6609        |
|    total_timesteps      | 3211264     |
| train/                  |             |
|    approx_kl            | 0.012709956 |
|    clip_fraction        | 0.317       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.000301    |
|    loss                 | 0.104       |
|    n_updates            | 780         |
|    policy_gradient_loss | 0.00261     |
|    std                  | 0.252       |
|    value_loss           | 0.151       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 486         |
|    iterations           | 197         |
|    time_elapsed         | 6638        |
|    total_timesteps      | 3227648     |
| train/                  |             |
|    approx_kl            | 0.014432359 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.297      |
|    explained_variance   | 0.865       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0531      |
|    n_updates            | 784         |
|    policy_gradient_loss | 0.00154     |
|    std                  | 0.251       |
|    value_loss           | 0.111       |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 486        |
|    iterations           | 198        |
|    time_elapsed         | 6666       |
|    total_timesteps      | 3244032    |
| train/                  |            |
|    approx_kl            | 0.01681254 |
|    clip_fraction        | 0.315      |
|    clip_range           | 0.15       |
|    clip_range_vf        | 0.15       |
|    entropy_loss         | -0.278     |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.000299   |
|    loss                 | 0.0701     |
|    n_updates            | 788        |
|    policy_gradient_loss | 0.000802   |
|    std                  | 0.251      |
|    value_loss           | 0.133      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 487         |
|    iterations           | 199         |
|    time_elapsed         | 6694        |
|    total_timesteps      | 3260416     |
| train/                  |             |
|    approx_kl            | 0.015116963 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.261      |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.000298    |
|    loss                 | 0.113       |
|    n_updates            | 792         |
|    policy_gradient_loss | 0.00048     |
|    std                  | 0.25        |
|    value_loss           | 0.175       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 487         |
|    iterations           | 200         |
|    time_elapsed         | 6723        |
|    total_timesteps      | 3276800     |
| train/                  |             |
|    approx_kl            | 0.016902618 |
|    clip_fraction        | 0.349       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.245      |
|    explained_variance   | 0.285       |
|    learning_rate        | 0.000297    |
|    loss                 | 0.0703      |
|    n_updates            | 796         |
|    policy_gradient_loss | 0.00155     |
|    std                  | 0.25        |
|    value_loss           | 0.129       |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 487          |
|    iterations           | 201          |
|    time_elapsed         | 6752         |
|    total_timesteps      | 3293184      |
| train/                  |              |
|    approx_kl            | 0.0143258525 |
|    clip_fraction        | 0.306        |
|    clip_range           | 0.15         |
|    clip_range_vf        | 0.15         |
|    entropy_loss         | -0.226       |
|    explained_variance   | 0.9          |
|    learning_rate        | 0.000296     |
|    loss                 | 0.0571       |
|    n_updates            | 800          |
|    policy_gradient_loss | -0.000879    |
|    std                  | 0.249        |
|    value_loss           | 0.133        |
------------------------------------------

---------------------------------------
Using our CustomActorCriticPolicy!
---------------------------------------


---------------------------------------
Starting model evaluation, checkpoint 33
---------------------------------------


---------------------------------------
Deleting Model:
Min walked distance: 15m
Mean walked distance: 15m
Mean step reward: 0.8245010852813721
Runs below 20m: []
---------------------------------------


---------------------------------------
WE COULD FINISH TRAINING EARLY!
Agent learned to stably walk after 3301495 stepswith mean step reward of 0.8245010852813721!
---------------------------------------

-----------------------------------------
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 202         |
|    time_elapsed         | 6848        |
|    total_timesteps      | 3309568     |
| train/                  |             |
|    approx_kl            | 0.015179054 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.927       |
|    learning_rate        | 0.000295    |
|    loss                 | 0.0739      |
|    n_updates            | 804         |
|    policy_gradient_loss | 0.000361    |
|    std                  | 0.249       |
|    value_loss           | 0.131       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 483         |
|    iterations           | 203         |
|    time_elapsed         | 6876        |
|    total_timesteps      | 3325952     |
| train/                  |             |
|    approx_kl            | 0.014845232 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.931       |
|    learning_rate        | 0.000294    |
|    loss                 | 0.0531      |
|    n_updates            | 808         |
|    policy_gradient_loss | 0.00159     |
|    std                  | 0.248       |
|    value_loss           | 0.116       |
-----------------------------------------
----------------------------------------
| time/                   |            |
|    fps                  | 484        |
|    iterations           | 204        |
|    time_elapsed         | 6905       |
|    total_timesteps      | 3342336    |
| train/                  |            |
|    approx_kl            | 0.01974865 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.15       |
|    clip_range_vf        | 0.15       |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.825      |
|    learning_rate        | 0.000293   |
|    loss                 | 0.073      |
|    n_updates            | 812        |
|    policy_gradient_loss | 0.000275   |
|    std                  | 0.247      |
|    value_loss           | 0.141      |
----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 484         |
|    iterations           | 205         |
|    time_elapsed         | 6934        |
|    total_timesteps      | 3358720     |
| train/                  |             |
|    approx_kl            | 0.011900868 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.000292    |
|    loss                 | 0.0749      |
|    n_updates            | 816         |
|    policy_gradient_loss | -0.00104    |
|    std                  | 0.246       |
|    value_loss           | 0.139       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 484         |
|    iterations           | 206         |
|    time_elapsed         | 6962        |
|    total_timesteps      | 3375104     |
| train/                  |             |
|    approx_kl            | 0.015246032 |
|    clip_fraction        | 0.329       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.00029     |
|    loss                 | 0.101       |
|    n_updates            | 820         |
|    policy_gradient_loss | -0.000623   |
|    std                  | 0.245       |
|    value_loss           | 0.167       |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 485         |
|    iterations           | 207         |
|    time_elapsed         | 6992        |
|    total_timesteps      | 3391488     |
| train/                  |             |
|    approx_kl            | 0.016261123 |
|    clip_fraction        | 0.303       |
|    clip_range           | 0.15        |
|    clip_range_vf        | 0.15        |
|    entropy_loss         | -0.0798     |
|    explained_variance   | 0.906       |
|    learning_rate        | 0.000289    |
|    loss                 | 0.0667      |
|    n_updates            | 824         |
|    policy_gradient_loss | 0.00175     |
|    std                  | 0.245       |
|    value_loss           | 0.115       |
-----------------------------------------
e_path, FINAL_CHECKPOINT_SUFFIX)

    # close environment
    env.close()

    # evaluate the trained model
^CTraceback (most recent call last):
  File "/home/asm01/Honors_Project/DRLoco-master/train.py", line 146, in <module>
    train()
  File "/home/asm01/Honors_Project/DRLoco-master/train.py", line 133, in train
    model.learn(total_timesteps=training_timesteps, callback=TrainingMonitor())
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py", line 317, in learn
    return super().learn(
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 262, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 187, in collect_rollouts
    if callback.on_step() is False:
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/stable_baselines3/common/callbacks.py", line 100, in on_step
    return self._on_step()
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/common/callback.py", line 85, in _on_step
    walking_stably = self.eval_walking()
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/common/callback.py", line 285, in eval_walking
    eval_env = utils.load_env(checkpoint, cfg.save_path, cfgl.ENV_ID)
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/common/utils.py", line 237, in load_env
    env = vec_env(env_id, num_envs=1, norm_rew=False, load_path=env_path)
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/common/utils.py", line 122, in vec_env
    vec_env = DummyVecEnv([make_env_func(seed, 0)])
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in __init__
    self.envs = [fn() for fn in env_fns]
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 25, in <listcomp>
    self.envs = [fn() for fn in env_fns]
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/common/utils.py", line 112, in make_env
    env = env_map[env_id]()
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/mujoco/mimic_walker3d.py", line 36, in __init__
    reference_trajectories = refs.StraightWalkingTrajectories(qpos_indices, qvel_indices)
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/ref_trajecs/straight_walk_trajecs.py", line 103, in __init__
    super(StraightWalkingTrajectories, self).__init__(
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/ref_trajecs/base_ref_trajecs.py", line 30, in __init__
    self._qpos_full, self._qvel_full = self._load_ref_trajecs()
  File "/home/asm01/Honors_Project/DRLoco-master/drloco/ref_trajecs/straight_walk_trajecs.py", line 309, in _load_ref_trajecs
    data = spio.loadmat(path, squeeze_me=True)
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/io/matlab/_mio.py", line 226, in loadmat
    matfile_dict = MR.get_variables(variable_names)
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/io/matlab/_mio5.py", line 332, in get_variables
    res = self.read_var_array(hdr, process)
  File "/home/asm01/anaconda3/envs/py39/lib/python3.9/site-packages/scipy/io/matlab/_mio5.py", line 292, in read_var_array
    return self._matrix_reader.array_from_header(header, process)
  File "_mio5_utils.pyx", line 671, in scipy.io.matlab._mio5_utils.VarReader5.array_from_header
  File "_mio5_utils.pyx", line 717, in scipy.io.matlab._mio5_utils.VarReader5.array_from_header
  File "_mio5_utils.pyx", line 891, in scipy.io.matlab._mio5_utils.VarReader5.read_cells
  File "_mio5_utils.pyx", line 669, in scipy.io.matlab._mio5_utils.VarReader5.read_mi_matrix
  File "_mio5_utils.pyx", line 717, in scipy.io.matlab._mio5_utils.VarReader5.array_from_header
  File "_mio5_utils.pyx", line 891, in scipy.io.matlab._mio5_utils.VarReader5.read_cells
  File "_mio5_utils.pyx", line 669, in scipy.io.matlab._mio5_utils.VarReader5.read_mi_matrix
  File "_mio5_utils.pyx", line 741, in scipy.io.matlab._mio5_utils.VarReader5.array_from_header
  File "_mio_utils.pyx", line 11, in scipy.io.matlab._mio_utils.squeeze_element
  File "_mio_utils.pyx", line 18, in scipy.io.matlab._mio_utils.squeeze_element
  File "<__array_function__ internals>", line 177, in squeeze
KeyboardInterrupt
wandb: Waiting for W&B process to finish... (failed 255). Press Control-C to abort syncing.
wandb: 
wandb: Run history:
wandb:              global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb: log_steps_to_convergence ‚ñÅ
wandb: 
wandb: Run summary:
wandb:              global_step 3401485
wandb: log_steps_to_convergence 2000312
wandb:     steps_to_convergence 2000312
wandb: 
wandb: üöÄ View run CPU, PHASE, MIRR_PY at: https://wandb.ai/lone_ranger/drloco_models/runs/paf8pzf2
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20230518_003159-paf8pzf2/logs

(py39) asm01@asm01-IdeaPad-Gaming3-15ARH05D:~/Honors_Project/DRLoco-master$ 
(py39) asm01@asm01-IdeaPad-Gaming3-15ARH05D:~/Honors_Project/DRLoco-master$ 
